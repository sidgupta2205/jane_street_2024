{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dad794da",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-17T21:33:57.882584Z",
     "iopub.status.busy": "2024-12-17T21:33:57.882289Z",
     "iopub.status.idle": "2024-12-17T21:34:02.884568Z",
     "shell.execute_reply": "2024-12-17T21:34:02.883861Z"
    },
    "papermill": {
     "duration": 5.016252,
     "end_time": "2024-12-17T21:34:02.886536",
     "exception": false,
     "start_time": "2024-12-17T21:33:57.870284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics as stat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import copy\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "evaluate = False\n",
    "# model_path = \"/kaggle/input/custom_trans_random_epoch9/pytorch/default/1/torchcustom_random_epoch9.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39d23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acef514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97046065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:02.907082Z",
     "iopub.status.busy": "2024-12-17T21:34:02.906433Z",
     "iopub.status.idle": "2024-12-17T21:34:02.910039Z",
     "shell.execute_reply": "2024-12-17T21:34:02.909322Z"
    },
    "papermill": {
     "duration": 0.014979,
     "end_time": "2024-12-17T21:34:02.911625",
     "exception": false,
     "start_time": "2024-12-17T21:34:02.896646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "jane_street_real_time_market_data_forecasting_path = '/home/siddharth/jane_street_challenge_2024/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8dd746",
   "metadata": {
    "papermill": {
     "duration": 0.008126,
     "end_time": "2024-12-17T21:34:02.928071",
     "exception": false,
     "start_time": "2024-12-17T21:34:02.919945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## LSTM Architecture\n",
    "One of the major problem of LSTM is the overfitting, thus to try to prevent this a little we will add some gaussian noise to the model. This helps a little but not perfectly.\n",
    "### Gaussian Noise\n",
    "We will add noise during the training part only, this is because we dont want it to affect the test results, only prevent the model from overfitting on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aec25e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:02.945685Z",
     "iopub.status.busy": "2024-12-17T21:34:02.945025Z",
     "iopub.status.idle": "2024-12-17T21:34:02.949458Z",
     "shell.execute_reply": "2024-12-17T21:34:02.948749Z"
    },
    "papermill": {
     "duration": 0.014746,
     "end_time": "2024-12-17T21:34:02.950986",
     "exception": false,
     "start_time": "2024-12-17T21:34:02.936240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, std=0.1):\n",
    "        super().__init__()\n",
    "        self.std = std\n",
    "        self.training = False\n",
    "    def forward(self, x):\n",
    "        if self.training: \n",
    "            noise = torch.randn_like(x) * self.std\n",
    "            return x + noise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07372cf3",
   "metadata": {
    "papermill": {
     "duration": 0.008091,
     "end_time": "2024-12-17T21:34:02.967328",
     "exception": false,
     "start_time": "2024-12-17T21:34:02.959237",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Base LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e108727d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:02.984687Z",
     "iopub.status.busy": "2024-12-17T21:34:02.984414Z",
     "iopub.status.idle": "2024-12-17T21:34:02.988582Z",
     "shell.execute_reply": "2024-12-17T21:34:02.987801Z"
    },
    "papermill": {
     "duration": 0.01472,
     "end_time": "2024-12-17T21:34:02.990106",
     "exception": false,
     "start_time": "2024-12-17T21:34:02.975386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_dim, output_size, num_layers):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         # Store dimensions of the hidden state and number of layers\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.num_layers = num_layers\n",
    "\n",
    "#         # Apply Gaussian Noise to the input for regularization (assume GaussianNoise is defined elsewhere)\n",
    "#         self.noise = GaussianNoise(std=0.1)\n",
    "\n",
    "#         # Define LSTM layers with dropout between stacked layers\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size, \n",
    "#             hidden_dim, \n",
    "#             num_layers, \n",
    "#             batch_first=True, \n",
    "#             dropout=0.3\n",
    "#         )\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "#         # Fully connected layer to map the hidden state output to the desired output size\n",
    "#         self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "#         self.fc2 = nn.Linear(64, 128)\n",
    "#         self.fc3 = nn.Linear(128, 64)\n",
    "#         self.out = nn.Linear(64, output_size)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.unsqueeze(1)  # Add a singleton dimension for batch_first=True\n",
    "#         x = self.noise(x)  # Apply Gaussian noise\n",
    "\n",
    "#         # Initialize hidden state h0 and cell state c0 with zeros\n",
    "#         h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "#         c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "#         # Pass the input and the initial states through the LSTM layers\n",
    "#         out, _ = self.lstm(x, (h0, c0))\n",
    "#         out = self.dropout(out)\n",
    "\n",
    "#         # Pass the output of the last LSTM layer to the fully connected layer\n",
    "#         out = self.relu(self.fc1(out[:, -1, :]))\n",
    "#         out = self.relu(self.fc2(out))\n",
    "#         out = self.dropout(out)\n",
    "#         out = self.relu(self.fc3(out))\n",
    "#         out = self.dropout(out)\n",
    "#         out = self.out(out)\n",
    "        \n",
    "#         return out.squeeze()  # Squeeze the output to remove singleton dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8affc20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:03.007848Z",
     "iopub.status.busy": "2024-12-17T21:34:03.007586Z",
     "iopub.status.idle": "2024-12-17T21:34:03.012145Z",
     "shell.execute_reply": "2024-12-17T21:34:03.011305Z"
    },
    "papermill": {
     "duration": 0.015216,
     "end_time": "2024-12-17T21:34:03.013664",
     "exception": false,
     "start_time": "2024-12-17T21:34:02.998448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from ncps.torch import LTC  # Liquid Time-Constant layer from Torch-NCPS\n",
    "# from ncps.wirings import AutoNCP\n",
    "# import snntorch as snn  # SNN library\n",
    "# from snntorch import surrogate  # Surrogate gradient for spike training\n",
    "\n",
    "# # Spiking Neural Network Layer with snntorch\n",
    "# class SpikingLayer(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, beta=0.9):\n",
    "#         super(SpikingLayer, self).__init__()\n",
    "#         self.fc = nn.Linear(input_size, input_size*2)\n",
    "#         self.fc2 = nn.Linear(input_size*2, input_size*4)\n",
    "#         self.fc3 = nn.Linear(input_size*4, input_size*2)\n",
    "#         self.fc4 = nn.Linear(input_size*2, output_size)\n",
    "#         self.activation = nn.GELU()\n",
    "#         self.encoder = nn.Sequential(self.fc,self.activation,self.fc2,self.activation,self.fc3,self.activation,self.fc4)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # mem = self.lif.init_leaky()  # Initialize membrane potential\n",
    "#         # spike, mem = self.lif(self.fc(x), mem)  # Generate spikes\n",
    "#         return self.encoder(x)\n",
    "\n",
    "# # Combined Liquid-SNN Model\n",
    "# class LiquidSNN(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size, time_steps):\n",
    "#         super(LiquidSNN, self).__init__()\n",
    "#         # Liquid Layer using Torch-NCPS\n",
    "#         wiring = AutoNCP(79, hidden_size)  # 16 units, 1 motor neuron\n",
    "#         self.liquid_layer = LTC(input_size, wiring, batch_first=True)\n",
    "#         self.time_steps = time_steps\n",
    "\n",
    "#         # Spiking Layer using snntorch\n",
    "#         self.spiking_layer = SpikingLayer(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Prepare input for the liquid layer (batch-first for Torch-NCPS)\n",
    "#         batch_size = x.size(0)\n",
    "#         x = x.unsqueeze(1).repeat(1, self.time_steps, 1)  # Repeat for time steps\n",
    "        \n",
    "#         # Pass through the liquid layer\n",
    "#         h,state = self.liquid_layer(x)\n",
    "#         # print(state.shape,h.shape)\n",
    "#         # Aggregate time-dependent outputs\n",
    "#         h_avg = h.mean(dim=1)  # Average across the time dimension\n",
    "#         # print(h_avg.shape)/\n",
    "#         # Pass through spiking layer\n",
    "#         spikes = self.spiking_layer(h_avg)\n",
    "#         return spikes\n",
    "\n",
    "# # Example Usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     batch_size = 4096\n",
    "#     input_size = 79\n",
    "#     hidden_size = 32\n",
    "#     output_size = 1\n",
    "#     time_steps = 1\n",
    "\n",
    "#     # Initialize model\n",
    "#     model = LiquidSNN(input_size, hidden_size, output_size, time_steps)\n",
    "#     x = torch.rand(batch_size, input_size)\n",
    "    \n",
    "#     # Forward pass\n",
    "#     output = model(x)\n",
    "#     print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0259b257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:03.030831Z",
     "iopub.status.busy": "2024-12-17T21:34:03.030593Z",
     "iopub.status.idle": "2024-12-17T21:34:03.035547Z",
     "shell.execute_reply": "2024-12-17T21:34:03.034746Z"
    },
    "papermill": {
     "duration": 0.015353,
     "end_time": "2024-12-17T21:34:03.037102",
     "exception": false,
     "start_time": "2024-12-17T21:34:03.021749",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class CustomTransformerNN(nn.Module):\n",
    "#     def __init__(self, num_features=79, embed_dim=8, nhead=4, num_layers=2):\n",
    "#         super(CustomTransformerNN, self).__init__()\n",
    "\n",
    "#         # Simple ANN for features\n",
    "#         self.feature_embeddings =  nn.Linear(1, embed_dim)\n",
    "\n",
    "#         # Simple ANN for time_id and date_id\n",
    "#         self.time_embedding = nn.Sequential(\n",
    "#             nn.Linear(3, embed_dim),\n",
    "#             nn.BatchNorm1d(embed_dim)\n",
    "#         )\n",
    "#         self.date_embedding = nn.Sequential(\n",
    "#             nn.Linear(3, embed_dim),\n",
    "#             nn.BatchNorm1d(embed_dim)\n",
    "#         )\n",
    "\n",
    "#         # Transformer Encoder\n",
    "#         self.transformer = nn.TransformerEncoder(\n",
    "#             nn.TransformerEncoderLayer(d_model=embed_dim, nhead=nhead),\n",
    "#             num_layers=num_layers\n",
    "#         )\n",
    "\n",
    "#         # CNN-based autoencoder structure with final output layer\n",
    "#         self.cnn_encoder = nn.Sequential(\n",
    "#             nn.Conv1d(79, 64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm1d(64),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Conv1d(64, 32, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm1d(32),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "\n",
    "#         self.cnn_decoder = nn.Sequential(\n",
    "#             nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm1d(64),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Conv1d(64, 79, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm1d(79),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "\n",
    "#         self.output_layer = nn.Sequential(\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(79 * embed_dim, 32),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(32,1)\n",
    "#         )\n",
    "#         self.noise = GaussianNoise()\n",
    "\n",
    "#     def forward(self, features, time_id, date_id):\n",
    "#         # Apply linear layers to features\n",
    "#         embedded_features = self.feature_embeddings(features.unsqueeze(dim=2))\n",
    "#         embedded_features = self.noise(embedded_features)\n",
    "#         # Apply linear layers to time_id and date_id\n",
    "#         time_encoding = self.time_embedding(time_id)  # Shape: (Batch, 8)\n",
    "#         date_encoding = self.date_embedding(date_id)  # Shape: (Batch, 8)\n",
    "\n",
    "#         # Add positional encodings to the features\n",
    "#         positional_encoding = time_encoding + date_encoding  # Shape: (Batch, 8)\n",
    "#         positional_encoding = positional_encoding.unsqueeze(1).expand_as(embedded_features)  # (Batch, 79, 8)\n",
    "#         transformer_input = embedded_features + positional_encoding\n",
    "\n",
    "#         # Transformer Encoder\n",
    "#         # transformer_output = self.transformer(transformer_input)  # Shape: (Batch, 79, 8).\n",
    "\n",
    "#         # Reshape for CNN\n",
    "#         cnn_input = transformer_input # Shape: (Batch, 8, 79)\n",
    "#         encoded = self.cnn_encoder(cnn_input)  # Shape: (Batch, 32, 79)\n",
    "#         decoded = self.cnn_decoder(encoded)  # Shape: (Batch, 8, 79)\n",
    "\n",
    "#         # Flatten and pass through final output layer\n",
    "#         flattened_output = decoded.permute(0, 2, 1).contiguous().view(decoded.size(0), -1)  # Shape: (Batch, 79 * 8)\n",
    "#         output = self.output_layer(flattened_output)  # Shape: (Batch, 1)\n",
    "\n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b2cca5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:03.054717Z",
     "iopub.status.busy": "2024-12-17T21:34:03.054447Z",
     "iopub.status.idle": "2024-12-17T21:34:03.062347Z",
     "shell.execute_reply": "2024-12-17T21:34:03.061563Z"
    },
    "papermill": {
     "duration": 0.018315,
     "end_time": "2024-12-17T21:34:03.063817",
     "exception": false,
     "start_time": "2024-12-17T21:34:03.045502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=79, encoded_dim=32):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        \n",
    "        # Encoder: Maps input_dim to encoded_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, encoded_dim),\n",
    "            nn.BatchNorm1d(encoded_dim),\n",
    "            nn.SiLU()  # Swish activation\n",
    "        )\n",
    "\n",
    "        # Decoder: Maps encoded_dim back to input_dim\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoded_dim, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),  # Swish activation\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, input_dim) # Use Sigmoid to map output in the range [0, 1], modify if not required\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=79, encoded_dim=32, output_dim=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.noise =  GaussianNoise(std=0.4)\n",
    "        # MLP: Takes concatenated input_dim + encoded_dim and outputs a single value\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear( input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Tanh(),  # Swish activation\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear( 128, 192),\n",
    "            nn.BatchNorm1d(192),\n",
    "            nn.Tanh(),  # Swish activation\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(192, 16),\n",
    "            # nn.BatchNorm1d(64),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(16, 1)  # Swish activation\n",
    "            # nn.Dropout(0.2),\n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x,training = True):\n",
    "        if training:\n",
    "            x = self.noise(x)\n",
    "        return self.mlp(x)*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "715cf884",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:03.080828Z",
     "iopub.status.busy": "2024-12-17T21:34:03.080355Z",
     "iopub.status.idle": "2024-12-17T21:34:03.083691Z",
     "shell.execute_reply": "2024-12-17T21:34:03.082916Z"
    },
    "papermill": {
     "duration": 0.01345,
     "end_time": "2024-12-17T21:34:03.085200",
     "exception": false,
     "start_time": "2024-12-17T21:34:03.071750",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input_size = 79\n",
    "# hidden_size = 32\n",
    "# output_size = 1\n",
    "# time_steps = 1\n",
    "# model = LiquidSNN(input_size, hidden_size, output_size, time_steps)\n",
    "# model.load_state_dict(torch.load('/kaggle/input/snn_liquid/pytorch/default/1/torchlstm.pth',weights_only=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caa9c0b",
   "metadata": {
    "papermill": {
     "duration": 0.008013,
     "end_time": "2024-12-17T21:34:03.101206",
     "exception": false,
     "start_time": "2024-12-17T21:34:03.093193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Data Normalization\n",
    "Found this on the forum of the competition, seemed to be extremely helpful for LSTM models.\n",
    "This is a collection of the means and standard deviations for all of the feature columns in the data and placed in a dictionnary for fast pulls.\n",
    "This makes the model better (increase R^2), but it also increases the overfitting, which is problematic for use on non-training data. However, like mentionned multiple times, overfitting is to be expected with LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5aa88bf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:03.118693Z",
     "iopub.status.busy": "2024-12-17T21:34:03.118448Z",
     "iopub.status.idle": "2024-12-17T21:34:03.131688Z",
     "shell.execute_reply": "2024-12-17T21:34:03.131033Z"
    },
    "papermill": {
     "duration": 0.023608,
     "end_time": "2024-12-17T21:34:03.133086",
     "exception": false,
     "start_time": "2024-12-17T21:34:03.109478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "means = {'feature_00': 0.640198826789856, 'feature_01': 0.03755598142743111, 'feature_02': 0.6368075609207153, 'feature_03': 0.6365063786506653, 'feature_04': 0.013741530478000641, 'feature_05': -0.02173694409430027, 'feature_06': -0.006415014620870352, 'feature_07': -0.010971736162900925, 'feature_08': -0.04653771221637726, 'feature_09': 32.596106194690265, 'feature_10': 4.95929203539823, 'feature_11': 167.6541592920354, 'feature_12': -0.13415881991386414, 'feature_13': -0.07573335617780685, 'feature_14': -0.12015637010335922, 'feature_15': -0.7470195889472961, 'feature_16': -0.6257441639900208, 'feature_17': -0.7294047474861145, 'feature_18': -0.042215555906295776, 'feature_19': -0.08798160403966904, 'feature_20': -0.15741558372974396, 'feature_21': 0.10528526455163956, 'feature_22': 0.018054703250527382, 'feature_23': 0.03165541961789131, 'feature_24': 2.733017921447754, 'feature_25': 0.39958420395851135, 'feature_26': -0.11045943945646286, 'feature_27': -0.5332594513893127, 'feature_28': -0.4522790312767029, 'feature_29': -0.5739678144454956, 'feature_30': -0.7905704975128174, 'feature_31': 0.10600688308477402, 'feature_32': 0.40044134855270386, 'feature_33': -0.021725023165345192, 'feature_34': 0.4226262867450714, 'feature_35': 0.42143046855926514, 'feature_36': -0.00023802756913937628, 'feature_37': 0.027961043640971184, 'feature_38': 0.010258913040161133, 'feature_39': 0.005768273025751114, 'feature_40': 0.017485467717051506, 'feature_41': 0.038347117602825165, 'feature_42': -0.06123563274741173, 'feature_43': -0.11644423753023148, 'feature_44': -0.12342483550310135, 'feature_45': -0.028769943863153458, 'feature_46': -0.015200662426650524, 'feature_47': 0.015717582777142525, 'feature_48': -0.0033910537604242563, 'feature_49': -0.0052393232472240925, 'feature_50': -0.2285808026790619, 'feature_51': -0.3548349440097809, 'feature_52': -0.358092725276947, 'feature_53': 0.2607136368751526, 'feature_54': 0.18796788156032562, 'feature_55': 0.3154229521751404, 'feature_56': -0.1471923440694809, 'feature_57': 0.15730056166648865, 'feature_58': -0.021774644032120705, 'feature_59': -0.0037768862675875425, 'feature_60': -0.010220836848020554, 'feature_61': -0.03178725391626358, 'feature_62': -0.3769100308418274, 'feature_63': -0.3229374587535858, 'feature_64': -0.3718394339084625, 'feature_65': -0.10233989357948303, 'feature_66': -0.13688170909881592, 'feature_67': -0.14402112364768982, 'feature_68': -0.06875362992286682, 'feature_69': -0.11862917989492416, 'feature_70': -0.11789549142122269, 'feature_71': -0.06013699993491173, 'feature_72': -0.10766122490167618, 'feature_73': -0.09921672940254211, 'feature_74': -0.10233042389154434, 'feature_75': -0.05991339311003685, 'feature_76': -0.06349952518939972, 'feature_77': -0.07424316555261612, 'feature_78': -0.07759837061166763}\n",
    "stds = {'feature_00': 1.027751088142395, 'feature_01': 1.0967519283294678, 'feature_02': 1.0156300067901611, 'feature_03': 1.0170334577560425, 'feature_04': 1.0726385116577148, 'feature_05': 0.9639211297035217, 'feature_06': 1.0963259935379028, 'feature_07': 1.0789952278137207, 'feature_08': 0.7962697148323059, 'feature_09': 23.72976726545254, 'feature_10': 3.1867162933797224, 'feature_11': 163.44513161352285, 'feature_12': 0.6700984835624695, 'feature_13': 0.5805172920227051, 'feature_14': 0.664044201374054, 'feature_15': 0.37517768144607544, 'feature_16': 0.3393096327781677, 'feature_17': 0.3603287935256958, 'feature_18': 0.9911752939224243, 'feature_19': 1.0550744533538818, 'feature_20': 0.6643751263618469, 'feature_21': 0.38239365816116333, 'feature_22': 0.950261116027832, 'feature_23': 0.8119344711303711, 'feature_24': 1.4362775087356567, 'feature_25': 1.0947270393371582, 'feature_26': 1.077124834060669, 'feature_27': 1.0645726919174194, 'feature_28': 1.0676648616790771, 'feature_29': 0.2640742361545563, 'feature_30': 0.19689509272575378, 'feature_31': 0.3815343976020813, 'feature_32': 1.2996565103530884, 'feature_33': 0.9989405870437622, 'feature_34': 1.3409572839736938, 'feature_35': 1.3365675210952759, 'feature_36': 0.8695492148399353, 'feature_37': 0.7334080934524536, 'feature_38': 0.698810338973999, 'feature_39': 0.7965824604034424, 'feature_40': 0.518515944480896, 'feature_41': 0.6384949088096619, 'feature_42': 0.8168442249298096, 'feature_43': 0.5228385925292969, 'feature_44': 0.6521403193473816, 'feature_45': 0.8666537404060364, 'feature_46': 0.9039222002029419, 'feature_47': 3.2711963653564453, 'feature_48': 0.6570901274681091, 'feature_49': 0.7083076238632202, 'feature_50': 1.0132617950439453, 'feature_51': 0.6081287860870361, 'feature_52': 0.9250587224960327, 'feature_53': 1.0421689748764038, 'feature_54': 0.5859629511833191, 'feature_55': 0.9191848039627075, 'feature_56': 0.9549097418785095, 'feature_57': 1.0204777717590332, 'feature_58': 0.8327276110649109, 'feature_59': 0.8309783339500427, 'feature_60': 0.8389413356781006, 'feature_61': 1.192766547203064, 'feature_62': 1.388945460319519, 'feature_63': 0.09957146644592285, 'feature_64': 0.3396177291870117, 'feature_65': 1.01683509349823, 'feature_66': 1.0824761390686035, 'feature_67': 0.642227828502655, 'feature_68': 0.5312599539756775, 'feature_69': 0.6208390593528748, 'feature_70': 0.6724499464035034, 'feature_71': 0.5356909036636353, 'feature_72': 0.6534596681594849, 'feature_73': 1.0855497121810913, 'feature_74': 1.0880277156829834, 'feature_75': 1.2321789264678955, 'feature_76': 1.2345560789108276, 'feature_77': 1.0921478271484375, 'feature_78': 1.0924347639083862}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c313ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:03.150151Z",
     "iopub.status.busy": "2024-12-17T21:34:03.149656Z",
     "iopub.status.idle": "2024-12-17T21:34:03.153176Z",
     "shell.execute_reply": "2024-12-17T21:34:03.152557Z"
    },
    "papermill": {
     "duration": 0.013548,
     "end_time": "2024-12-17T21:34:03.154646",
     "exception": false,
     "start_time": "2024-12-17T21:34:03.141098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "m_arr = np.array(list(means.values()))\n",
    "stds_arr = np.array(list(stds.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd31ea69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:03.171788Z",
     "iopub.status.busy": "2024-12-17T21:34:03.171539Z",
     "iopub.status.idle": "2024-12-17T21:34:03.185438Z",
     "shell.execute_reply": "2024-12-17T21:34:03.184656Z"
    },
    "papermill": {
     "duration": 0.024155,
     "end_time": "2024-12-17T21:34:03.186933",
     "exception": false,
     "start_time": "2024-12-17T21:34:03.162778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "means = {'feature_00': 0.640198826789856, 'feature_01': 0.03755598142743111, 'feature_02': 0.6368075609207153, 'feature_03': 0.6365063786506653, 'feature_04': 0.013741530478000641, 'feature_05': -0.02173694409430027, 'feature_06': -0.006415014620870352, 'feature_07': -0.010971736162900925, 'feature_08': -0.04653771221637726, 'feature_09': 32.596106194690265, 'feature_10': 4.95929203539823, 'feature_11': 167.6541592920354, 'feature_12': -0.13415881991386414, 'feature_13': -0.07573335617780685, 'feature_14': -0.12015637010335922, 'feature_15': -0.7470195889472961, 'feature_16': -0.6257441639900208, 'feature_17': -0.7294047474861145, 'feature_18': -0.042215555906295776, 'feature_19': -0.08798160403966904, 'feature_20': -0.15741558372974396, 'feature_21': 0.10528526455163956, 'feature_22': 0.018054703250527382, 'feature_23': 0.03165541961789131, 'feature_24': 2.733017921447754, 'feature_25': 0.39958420395851135, 'feature_26': -0.11045943945646286, 'feature_27': -0.5332594513893127, 'feature_28': -0.4522790312767029, 'feature_29': -0.5739678144454956, 'feature_30': -0.7905704975128174, 'feature_31': 0.10600688308477402, 'feature_32': 0.40044134855270386, 'feature_33': -0.021725023165345192, 'feature_34': 0.4226262867450714, 'feature_35': 0.42143046855926514, 'feature_36': -0.00023802756913937628, 'feature_37': 0.027961043640971184, 'feature_38': 0.010258913040161133, 'feature_39': 0.005768273025751114, 'feature_40': 0.017485467717051506, 'feature_41': 0.038347117602825165, 'feature_42': -0.06123563274741173, 'feature_43': -0.11644423753023148, 'feature_44': -0.12342483550310135, 'feature_45': -0.028769943863153458, 'feature_46': -0.015200662426650524, 'feature_47': 0.015717582777142525, 'feature_48': -0.0033910537604242563, 'feature_49': -0.0052393232472240925, 'feature_50': -0.2285808026790619, 'feature_51': -0.3548349440097809, 'feature_52': -0.358092725276947, 'feature_53': 0.2607136368751526, 'feature_54': 0.18796788156032562, 'feature_55': 0.3154229521751404, 'feature_56': -0.1471923440694809, 'feature_57': 0.15730056166648865, 'feature_58': -0.021774644032120705, 'feature_59': -0.0037768862675875425, 'feature_60': -0.010220836848020554, 'feature_61': -0.03178725391626358, 'feature_62': -0.3769100308418274, 'feature_63': -0.3229374587535858, 'feature_64': -0.3718394339084625, 'feature_65': -0.10233989357948303, 'feature_66': -0.13688170909881592, 'feature_67': -0.14402112364768982, 'feature_68': -0.06875362992286682, 'feature_69': -0.11862917989492416, 'feature_70': -0.11789549142122269, 'feature_71': -0.06013699993491173, 'feature_72': -0.10766122490167618, 'feature_73': -0.09921672940254211, 'feature_74': -0.10233042389154434, 'feature_75': -0.05991339311003685, 'feature_76': -0.06349952518939972, 'feature_77': -0.07424316555261612, 'feature_78': -0.07759837061166763}\n",
    "stds = {'feature_00': 1.027751088142395, 'feature_01': 1.0967519283294678, 'feature_02': 1.0156300067901611, 'feature_03': 1.0170334577560425, 'feature_04': 1.0726385116577148, 'feature_05': 0.9639211297035217, 'feature_06': 1.0963259935379028, 'feature_07': 1.0789952278137207, 'feature_08': 0.7962697148323059, 'feature_09': 23.72976726545254, 'feature_10': 3.1867162933797224, 'feature_11': 163.44513161352285, 'feature_12': 0.6700984835624695, 'feature_13': 0.5805172920227051, 'feature_14': 0.664044201374054, 'feature_15': 0.37517768144607544, 'feature_16': 0.3393096327781677, 'feature_17': 0.3603287935256958, 'feature_18': 0.9911752939224243, 'feature_19': 1.0550744533538818, 'feature_20': 0.6643751263618469, 'feature_21': 0.38239365816116333, 'feature_22': 0.950261116027832, 'feature_23': 0.8119344711303711, 'feature_24': 1.4362775087356567, 'feature_25': 1.0947270393371582, 'feature_26': 1.077124834060669, 'feature_27': 1.0645726919174194, 'feature_28': 1.0676648616790771, 'feature_29': 0.2640742361545563, 'feature_30': 0.19689509272575378, 'feature_31': 0.3815343976020813, 'feature_32': 1.2996565103530884, 'feature_33': 0.9989405870437622, 'feature_34': 1.3409572839736938, 'feature_35': 1.3365675210952759, 'feature_36': 0.8695492148399353, 'feature_37': 0.7334080934524536, 'feature_38': 0.698810338973999, 'feature_39': 0.7965824604034424, 'feature_40': 0.518515944480896, 'feature_41': 0.6384949088096619, 'feature_42': 0.8168442249298096, 'feature_43': 0.5228385925292969, 'feature_44': 0.6521403193473816, 'feature_45': 0.8666537404060364, 'feature_46': 0.9039222002029419, 'feature_47': 3.2711963653564453, 'feature_48': 0.6570901274681091, 'feature_49': 0.7083076238632202, 'feature_50': 1.0132617950439453, 'feature_51': 0.6081287860870361, 'feature_52': 0.9250587224960327, 'feature_53': 1.0421689748764038, 'feature_54': 0.5859629511833191, 'feature_55': 0.9191848039627075, 'feature_56': 0.9549097418785095, 'feature_57': 1.0204777717590332, 'feature_58': 0.8327276110649109, 'feature_59': 0.8309783339500427, 'feature_60': 0.8389413356781006, 'feature_61': 1.192766547203064, 'feature_62': 1.388945460319519, 'feature_63': 0.09957146644592285, 'feature_64': 0.3396177291870117, 'feature_65': 1.01683509349823, 'feature_66': 1.0824761390686035, 'feature_67': 0.642227828502655, 'feature_68': 0.5312599539756775, 'feature_69': 0.6208390593528748, 'feature_70': 0.6724499464035034, 'feature_71': 0.5356909036636353, 'feature_72': 0.6534596681594849, 'feature_73': 1.0855497121810913, 'feature_74': 1.0880277156829834, 'feature_75': 1.2321789264678955, 'feature_76': 1.2345560789108276, 'feature_77': 1.0921478271484375, 'feature_78': 1.0924347639083862}\n",
    "\n",
    "def normalize_dataframe(df: pl.DataFrame, means: dict, stds: dict) -> pl.DataFrame:\n",
    "    # We normalize the polars dataframe using the provided means and standard deviations\n",
    "    normalize_exprs = []\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col in means and col in stds: #only normalize columns present in the means and std\n",
    "            if stds[col] != 0: #avoid division by 0\n",
    "                #Normalize the column and alias it with the same name\n",
    "                normalize_exprs.append(\n",
    "                    ((pl.col(col) - means[col]) / stds[col]).alias(col)\n",
    "                )\n",
    "            else:\n",
    "                normalize_exprs.append(pl.col(col) - means[col]).alias(col)\n",
    "\n",
    "    normalized_df = df.select(normalize_exprs) #Dataframe with normalized expressions\n",
    "    return normalized_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfb59a",
   "metadata": {
    "papermill": {
     "duration": 0.008007,
     "end_time": "2024-12-17T21:34:03.202973",
     "exception": false,
     "start_time": "2024-12-17T21:34:03.194966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preparing the Training Data\n",
    "Date to start the training data from, data collection gotten from https://www.kaggle.com/code/chumajin/janestreet-updated-simulator-for-time-series-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ff3090f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:03.219958Z",
     "iopub.status.busy": "2024-12-17T21:34:03.219716Z",
     "iopub.status.idle": "2024-12-17T21:34:03.223297Z",
     "shell.execute_reply": "2024-12-17T21:34:03.222510Z"
    },
    "papermill": {
     "duration": 0.013729,
     "end_time": "2024-12-17T21:34:03.224771",
     "exception": false,
     "start_time": "2024-12-17T21:34:03.211042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_range = [1455,1672]\n",
    "val_range = [1673,1693]\n",
    "test_range = [1694,1698]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1320945a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:03.242129Z",
     "iopub.status.busy": "2024-12-17T21:34:03.241474Z",
     "iopub.status.idle": "2024-12-17T21:34:03.245889Z",
     "shell.execute_reply": "2024-12-17T21:34:03.245096Z"
    },
    "papermill": {
     "duration": 0.014652,
     "end_time": "2024-12-17T21:34:03.247438",
     "exception": false,
     "start_time": "2024-12-17T21:34:03.232786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_vector(date_ids, time_ids):\n",
    "    date_ids = np.array(date_ids).reshape(-1, 1)\n",
    "    time_ids = np.array(time_ids).reshape(-1, 1)\n",
    "    # print(date_ids)/\n",
    "    # print(data_ids.shape)\n",
    "    date_vectors = np.hstack((date_ids / 30, date_ids / 12, date_ids / 365))\n",
    "    # print(date_vectors)\n",
    "    # print(\"hrerer\")\n",
    "    time_vectors = np.hstack((time_ids / 16, time_ids / 60, time_ids / 360))\n",
    "    \n",
    "    return date_vectors, time_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52ef1bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:03.264449Z",
     "iopub.status.busy": "2024-12-17T21:34:03.264191Z",
     "iopub.status.idle": "2024-12-17T21:34:03.268907Z",
     "shell.execute_reply": "2024-12-17T21:34:03.268118Z"
    },
    "papermill": {
     "duration": 0.015083,
     "end_time": "2024-12-17T21:34:03.270536",
     "exception": false,
     "start_time": "2024-12-17T21:34:03.255453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_features(train):\n",
    "    feature_names = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "    train_features = train.select(feature_names)\n",
    "    train_features = train_features.fill_null(strategy='forward').fill_null(0)\n",
    "    \n",
    "    # date_ids,time_ids = convert_to_vector(date_ids, time_ids)\n",
    "    # train_features = normalize_dataframe(train_features,means,stds)\n",
    "    X = train_features.to_numpy()\n",
    "    del train_features\n",
    "    y = train.select('responder_6').to_numpy().reshape(-1)\n",
    "    weights = train.select('weight').to_numpy().reshape(-1)\n",
    "\n",
    "    return X,y,weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faf39463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d7619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "618deb30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:51.224088Z",
     "iopub.status.busy": "2024-12-17T21:34:51.223814Z",
     "iopub.status.idle": "2024-12-17T21:34:51.227114Z",
     "shell.execute_reply": "2024-12-17T21:34:51.226405Z"
    },
    "papermill": {
     "duration": 0.013685,
     "end_time": "2024-12-17T21:34:51.228583",
     "exception": false,
     "start_time": "2024-12-17T21:34:51.214898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# date_ids_train.shape,weights_train.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e952f865",
   "metadata": {
    "papermill": {
     "duration": 0.007922,
     "end_time": "2024-12-17T21:34:51.244691",
     "exception": false,
     "start_time": "2024-12-17T21:34:51.236769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Train Validation Test Split\n",
    "Use 80% of the data for training 10% for validation and early stopping, 10% for final testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81b6e14c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:51.263479Z",
     "iopub.status.busy": "2024-12-17T21:34:51.262974Z",
     "iopub.status.idle": "2024-12-17T21:34:51.266734Z",
     "shell.execute_reply": "2024-12-17T21:34:51.266011Z"
    },
    "papermill": {
     "duration": 0.01546,
     "end_time": "2024-12-17T21:34:51.268246",
     "exception": false,
     "start_time": "2024-12-17T21:34:51.252786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import gc\n",
    "\n",
    "alltraindata = pl.scan_parquet(f\"{jane_street_real_time_market_data_forecasting_path}/train.parquet\")\n",
    "\n",
    "# Get unique date_ids greater than 1400 and shuffle them\n",
    "unique_date_ids = alltraindata.select(\"date_id\").unique().collect().to_series().to_list()\n",
    "unique_date_ids = [date_id for date_id in unique_date_ids if date_id > 1400]\n",
    "\n",
    "# Separate dates 1690 to 1698 for the test set\n",
    "test_date_ids_specific = [date_id for date_id in unique_date_ids if 1690 <= date_id <= 1698]\n",
    "remaining_date_ids = [date_id for date_id in unique_date_ids if date_id not in test_date_ids_specific]\n",
    "random.shuffle(remaining_date_ids)\n",
    "\n",
    "# Split remaining date_ids into train and val subsets\n",
    "train_size = int(len(remaining_date_ids) * 0.7)  # 70% for training\n",
    "val_size = len(remaining_date_ids) - train_size  # Remaining for validation\n",
    "\n",
    "train_date_ids = remaining_date_ids[:train_size]\n",
    "val_date_ids = remaining_date_ids[train_size:]\n",
    "test_date_ids = test_date_ids_specific\n",
    "\n",
    "# Filter data based on date_id splits\n",
    "train = alltraindata.filter(pl.col(\"date_id\").is_in(train_date_ids)).collect()\n",
    "X_train, y_train, weights_train  = get_features(train)\n",
    "del train\n",
    "gc.collect()\n",
    "val = alltraindata.filter(pl.col(\"date_id\").is_in(val_date_ids)).collect()\n",
    "X_val, y_val, weights_val = get_features(val)\n",
    "del val\n",
    "gc.collect()\n",
    "test = alltraindata.filter(pl.col(\"date_id\").is_in(test_date_ids)).collect()\n",
    "X_test, y_test, weights_test = get_features(test)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ccf212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4c1c99b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:51.285893Z",
     "iopub.status.busy": "2024-12-17T21:34:51.285422Z",
     "iopub.status.idle": "2024-12-17T21:34:51.358341Z",
     "shell.execute_reply": "2024-12-17T21:34:51.357624Z"
    },
    "papermill": {
     "duration": 0.083202,
     "end_time": "2024-12-17T21:34:51.359792",
     "exception": false,
     "start_time": "2024-12-17T21:34:51.276590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "345dc5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:51.377344Z",
     "iopub.status.busy": "2024-12-17T21:34:51.376848Z",
     "iopub.status.idle": "2024-12-17T21:34:58.419773Z",
     "shell.execute_reply": "2024-12-17T21:34:58.418920Z"
    },
    "papermill": {
     "duration": 7.053649,
     "end_time": "2024-12-17T21:34:58.421645",
     "exception": false,
     "start_time": "2024-12-17T21:34:51.367996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7436176, 79])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X = torch.tensor(X_train,dtype=torch.float16).to(device)\n",
    "train_y = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "train_weights = torch.tensor(weights_train,dtype=torch.float32).to(device)\n",
    "# train_time_ids = torch.tensor(date_ids_train,dtype=torch.float32).to(device)\n",
    "# train_date_ids = torch.tensor(time_ids_train,dtype=torch.float32).to(device)\n",
    "\n",
    "val_X = torch.tensor(X_val,dtype=torch.float32).to(device)\n",
    "val_y = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "val_weights = torch.tensor(weights_val,dtype=torch.float32).to(device)\n",
    "# val_time_ids = torch.tensor(date_ids_val,dtype=torch.float32).to(device)\n",
    "# val_date_ids = torch.tensor(time_ids_val,dtype=torch.float32).to(device)\n",
    "\n",
    "test_X = torch.tensor(X_test,dtype=torch.float32).to(device)\n",
    "test_y = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "test_weights = torch.tensor(weights_test,dtype=torch.float32).to(device)\n",
    "# test_time_ids = torch.tensor(date_ids_test,dtype=torch.float32).to(device)\n",
    "# test_date_ids = torch.tensor(time_ids_test,dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d167b577",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.440221Z",
     "iopub.status.busy": "2024-12-17T21:34:58.439908Z",
     "iopub.status.idle": "2024-12-17T21:34:58.445453Z",
     "shell.execute_reply": "2024-12-17T21:34:58.444662Z"
    },
    "papermill": {
     "duration": 0.016659,
     "end_time": "2024-12-17T21:34:58.447109",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.430450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_time_ids.shape,test_date_ids.shape\n",
    "\n",
    "m_arr = torch.tensor(m_arr,dtype=torch.float32).to(device)\n",
    "stds= torch.tensor(stds_arr,dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a5e0a71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.464971Z",
     "iopub.status.busy": "2024-12-17T21:34:58.464619Z",
     "iopub.status.idle": "2024-12-17T21:34:58.469881Z",
     "shell.execute_reply": "2024-12-17T21:34:58.469186Z"
    },
    "papermill": {
     "duration": 0.016046,
     "end_time": "2024-12-17T21:34:58.471537",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.455491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,X , y, weights):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.weights = weights\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx]-m_arr)/stds, self.y[idx],self.weights[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "755aa8fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.489409Z",
     "iopub.status.busy": "2024-12-17T21:34:58.488792Z",
     "iopub.status.idle": "2024-12-17T21:34:58.492751Z",
     "shell.execute_reply": "2024-12-17T21:34:58.492071Z"
    },
    "papermill": {
     "duration": 0.014401,
     "end_time": "2024-12-17T21:34:58.494193",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.479792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_X, train_y,train_weights)\n",
    "val_dataset = CustomDataset(val_X, val_y, val_weights)\n",
    "test_dataset = CustomDataset(test_X, test_y, test_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfcabaa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.511933Z",
     "iopub.status.busy": "2024-12-17T21:34:58.511423Z",
     "iopub.status.idle": "2024-12-17T21:34:58.515901Z",
     "shell.execute_reply": "2024-12-17T21:34:58.515154Z"
    },
    "papermill": {
     "duration": 0.014974,
     "end_time": "2024-12-17T21:34:58.517418",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.502444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size,shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4268e107",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.535575Z",
     "iopub.status.busy": "2024-12-17T21:34:58.535026Z",
     "iopub.status.idle": "2024-12-17T21:34:58.538849Z",
     "shell.execute_reply": "2024-12-17T21:34:58.538089Z"
    },
    "papermill": {
     "duration": 0.014603,
     "end_time": "2024-12-17T21:34:58.540552",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.525949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "run_type = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Local')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d55e9",
   "metadata": {
    "papermill": {
     "duration": 0.008561,
     "end_time": "2024-12-17T21:34:58.560949",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.552388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3472e087",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.578771Z",
     "iopub.status.busy": "2024-12-17T21:34:58.578277Z",
     "iopub.status.idle": "2024-12-17T21:34:58.581714Z",
     "shell.execute_reply": "2024-12-17T21:34:58.580970Z"
    },
    "papermill": {
     "duration": 0.014101,
     "end_time": "2024-12-17T21:34:58.583432",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.569331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder_path = \"/kaggle/input/encoder/pytorch/default/1/feature_learner.pth\"\n",
    "# encoder = AutoEncoder()\n",
    "# # model = nn.DataParallel(model)\n",
    "# encoder.to(device)\n",
    "# encoder.load_state_dict(torch.load(encoder_path, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4c6a53f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.603933Z",
     "iopub.status.busy": "2024-12-17T21:34:58.603420Z",
     "iopub.status.idle": "2024-12-17T21:34:58.607616Z",
     "shell.execute_reply": "2024-12-17T21:34:58.606860Z"
    },
    "papermill": {
     "duration": 0.015096,
     "end_time": "2024-12-17T21:34:58.609208",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.594112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if evaluate:\n",
    "    print(\"Loading pretrained weights\")\n",
    "    model.noise.training = False\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77ac8faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.626700Z",
     "iopub.status.busy": "2024-12-17T21:34:58.626225Z",
     "iopub.status.idle": "2024-12-17T21:34:58.630669Z",
     "shell.execute_reply": "2024-12-17T21:34:58.629936Z"
    },
    "papermill": {
     "duration": 0.014705,
     "end_time": "2024-12-17T21:34:58.632181",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.617476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weighted r2 gotten from https://www.kaggle.com/code/chumajin/janestreet-updated-simulator-for-time-series-api\n",
    "def r2_score(y_true, y_pred, weights):\n",
    "    \"\"\"\n",
    "    Calculate the sample weighted zero-mean R-squared score.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (numpy.ndarray): Ground-truth values for responder_6.\n",
    "    y_pred (numpy.ndarray): Predicted values for responder_6.\n",
    "    weights (numpy.ndarray): Sample weight vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The weighted zero-mean R-squared score.\n",
    "    \"\"\"\n",
    "    numerator = np.sum(weights * (y_true - y_pred)**2)\n",
    "    denominator = np.sum(weights * y_true**2)\n",
    "\n",
    "    r2_score = 1 - numerator / denominator\n",
    "    return r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be2ee85",
   "metadata": {
    "papermill": {
     "duration": 0.008058,
     "end_time": "2024-12-17T21:34:58.648441",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.640383",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1bfbfbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.666061Z",
     "iopub.status.busy": "2024-12-17T21:34:58.665577Z",
     "iopub.status.idle": "2024-12-17T21:34:58.668737Z",
     "shell.execute_reply": "2024-12-17T21:34:58.668065Z"
    },
    "papermill": {
     "duration": 0.013489,
     "end_time": "2024-12-17T21:34:58.670179",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.656690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "462700a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.687535Z",
     "iopub.status.busy": "2024-12-17T21:34:58.687316Z",
     "iopub.status.idle": "2024-12-17T21:34:58.694174Z",
     "shell.execute_reply": "2024-12-17T21:34:58.693507Z"
    },
    "papermill": {
     "duration": 0.017119,
     "end_time": "2024-12-17T21:34:58.695612",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.678493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def train_model(model, encoder,loader, optimizer, loss_function, device):\n",
    "    model.train() #set model to training mode\n",
    "    total_loss = 0\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "    all_weights = []\n",
    "    # model.noise.training = True\n",
    "    #Iterate over batches \n",
    "    progress_bar = tqdm(loader, desc=\"Training Progress\", leave=True,position =0)\n",
    "    for X_batch, y_batch, weights_batch in progress_bar:\n",
    "        #Move data to specified device (CPU or GPU)\n",
    "        # X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        # weights_batch = weights_batch.to(device)\n",
    "        #Reset Gradient to 0\n",
    "        optimizer.zero_grad()\n",
    "        # print(batch_time.shape,batch_date.shape)\n",
    "        # encoded,_ = encoder(X_batch)\n",
    "        # concat = torch.cat((X_batch, encoded), dim=1)\n",
    "        outputs = model(X_batch)\n",
    "        outputs = outputs.squeeze(dim=1)\n",
    "        y_batch = y_batch\n",
    "        loss_per_sample = loss_function(outputs, y_batch)\n",
    "        weighted_loss = loss_per_sample*weights_batch\n",
    "        #Compute average loss across the batch\n",
    "        loss = weighted_loss.mean()\n",
    "        # print(f\"Batch Loss: {loss.item():.4f}\")\n",
    "        progress_bar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        #Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        all_probs.append(outputs.detach().cpu())\n",
    "        all_targets.append(y_batch.cpu())\n",
    "        all_weights.append(weights_batch.cpu())\n",
    "\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    all_weights = torch.cat(all_weights).numpy()\n",
    "    mse = mean_squared_error(all_targets, all_probs, sample_weight=all_weights)\n",
    "    r2 = r2_score(all_targets, all_probs, all_weights)\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f65dadc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.712915Z",
     "iopub.status.busy": "2024-12-17T21:34:58.712462Z",
     "iopub.status.idle": "2024-12-17T21:34:58.718236Z",
     "shell.execute_reply": "2024-12-17T21:34:58.717559Z"
    },
    "papermill": {
     "duration": 0.016098,
     "end_time": "2024-12-17T21:34:58.719763",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.703665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, encoder,loader,generate_preds = False):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "    all_weights = []\n",
    "    # model.noise.training = False\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, weights_batch in tqdm(loader, desc=\"Validating Progress\", leave=True,position=0):\n",
    "            y_batch = y_batch\n",
    "            optimizer.zero_grad()\n",
    "            # print(batch_time.shape,batch_date.shape)\n",
    "            # encoded,_ = encoder(X_batch)\n",
    "            # concat = torch.cat((X_batch, encoded), dim=1)\n",
    "            outputs = model(X_batch,False)\n",
    "\n",
    "            all_probs.append(outputs.squeeze(dim=1).cpu())\n",
    "            all_targets.append(y_batch.cpu())\n",
    "            all_weights.append(weights_batch.cpu())\n",
    "\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    all_weights = torch.cat(all_weights).numpy()\n",
    "    \n",
    "    mse = mean_squared_error(all_targets, all_probs, sample_weight=all_weights)\n",
    "    r2 = r2_score(all_targets, all_probs, all_weights)\n",
    "    print(f\"Total Loss val: {mse:.4f} r2 {r2}\")\n",
    "    if generate_preds:\n",
    "        return all_targets,all_probs,mse,r2\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79b7adb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.737214Z",
     "iopub.status.busy": "2024-12-17T21:34:58.736745Z",
     "iopub.status.idle": "2024-12-17T21:34:58.740928Z",
     "shell.execute_reply": "2024-12-17T21:34:58.740011Z"
    },
    "papermill": {
     "duration": 0.01454,
     "end_time": "2024-12-17T21:34:58.742482",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.727942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "# def train_model(model, loader, optimizer, loss_function, device):\n",
    "#     model.train() #set model to training mode\n",
    "#     total_loss = 0\n",
    "#     all_probs = []\n",
    "#     all_targets = []\n",
    "#     all_weights = []\n",
    "#     model.noise.training = True\n",
    "#     #Iterate over batches \n",
    "#     progress_bar = tqdm(loader, desc=\"Training Progress\", leave=True,position =0)\n",
    "#     for X_batch, y_batch, batch_time,batch_date,weights_batch in progress_bar:\n",
    "#         #Move data to specified device (CPU or GPU)\n",
    "#         # X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "#         # weights_batch = weights_batch.to(device)\n",
    "#         #Reset Gradient to 0\n",
    "#         optimizer.zero_grad()\n",
    "#         # print(batch_time.shape,batch_date.shape)\n",
    "#         outputs = model(X_batch,batch_time,batch_date)\n",
    "#         outputs = outputs.squeeze(dim=1)\n",
    "#         loss_per_sample = loss_function(outputs, y_batch)\n",
    "#         weighted_loss = loss_per_sample\n",
    "#         #Compute average loss across the batch\n",
    "#         loss = weighted_loss\n",
    "#         # print(f\"Batch Loss: {loss.item():.4f}\")\n",
    "#         progress_bar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "#         loss.backward()\n",
    "\n",
    "#         #Update model parameters\n",
    "#         optimizer.step()\n",
    "\n",
    "#         total_loss += loss.item()\n",
    "\n",
    "#         all_probs.append(outputs.detach().cpu())\n",
    "#         all_targets.append(y_batch.cpu())\n",
    "#         all_weights.append(weights_batch.cpu())\n",
    "\n",
    "#     all_probs = torch.cat(all_probs).numpy()\n",
    "#     all_targets = torch.cat(all_targets).numpy()\n",
    "#     all_weights = torch.cat(all_weights).numpy()\n",
    "#     mse = mean_squared_error(all_targets, all_probs, sample_weight=all_weights)\n",
    "#     r2 = r2_score(all_targets, all_probs, all_weights)\n",
    "\n",
    "#     avg_loss = total_loss / len(loader)\n",
    "#     return avg_loss, mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecf811ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:34:58.760373Z",
     "iopub.status.busy": "2024-12-17T21:34:58.760090Z",
     "iopub.status.idle": "2024-12-17T21:35:01.770691Z",
     "shell.execute_reply": "2024-12-17T21:35:01.769693Z"
    },
    "papermill": {
     "duration": 3.022795,
     "end_time": "2024-12-17T21:35:01.773406",
     "exception": false,
     "start_time": "2024-12-17T21:34:58.750611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch,weights_batch in train_loader:\n",
    "    print(weights_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3e98af6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:01.791654Z",
     "iopub.status.busy": "2024-12-17T21:35:01.791325Z",
     "iopub.status.idle": "2024-12-17T21:35:01.796083Z",
     "shell.execute_reply": "2024-12-17T21:35:01.795429Z"
    },
    "papermill": {
     "duration": 0.015742,
     "end_time": "2024-12-17T21:35:01.797834",
     "exception": false,
     "start_time": "2024-12-17T21:35:01.782092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def evaluate_model(model, loader,generate_preds = False):\n",
    "#     model.eval()\n",
    "#     all_probs = []\n",
    "#     all_targets = []\n",
    "#     all_weights = []\n",
    "#     model.noise.training = False\n",
    "#     with torch.no_grad():\n",
    "#         for X_batch, y_batch, batch_time,batch_date,weights_batch in tqdm(loader, desc=\"Validating Progress\", leave=True,position=0):\n",
    "#             outputs = model(X_batch,batch_time,batch_date)\n",
    "\n",
    "#             all_probs.append(outputs.squeeze(dim=1).cpu())\n",
    "#             all_targets.append(y_batch.cpu())\n",
    "#             all_weights.append(weights_batch.cpu())\n",
    "\n",
    "#     all_probs = torch.cat(all_probs).numpy()\n",
    "#     all_targets = torch.cat(all_targets).numpy()\n",
    "#     all_weights = torch.cat(all_weights).numpy()\n",
    "    \n",
    "#     mse = mean_squared_error(all_targets, all_probs, sample_weight=all_weights)\n",
    "#     r2 = r2_score(all_targets, all_probs, all_weights)\n",
    "#     print(f\"Total Loss val: {mse:.4f} r2 {r2}\")\n",
    "#     if generate_preds:\n",
    "#         return all_targets,all_probs,mse,r2\n",
    "#     return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59399944",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:01.815643Z",
     "iopub.status.busy": "2024-12-17T21:35:01.815343Z",
     "iopub.status.idle": "2024-12-17T21:35:01.822213Z",
     "shell.execute_reply": "2024-12-17T21:35:01.821432Z"
    },
    "papermill": {
     "duration": 0.017502,
     "end_time": "2024-12-17T21:35:01.823822",
     "exception": false,
     "start_time": "2024-12-17T21:35:01.806320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def train_ae(model, loader, optimizer, loss_function, device):\n",
    "    model.train() #set model to training mode\n",
    "    #Iterate over batches \n",
    "    total_loss = 0\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "    all_weights = []\n",
    "    progress_bar = tqdm(loader, desc=\"Training Progress\", leave=True,position =0)\n",
    "    for X_batch, y_batch, batch_time,batch_date,weights_batch in progress_bar:\n",
    "        #Move data to specified device (CPU or GPU)\n",
    "        # X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        # weights_batch = weights_batch.to(device)\n",
    "        #Reset Gradient to 0\n",
    "        optimizer.zero_grad()\n",
    "        # print(batch_time.shape,batch_date.shape)\n",
    "        _,outputs = model(X_batch)\n",
    "        outputs = outputs\n",
    "        loss_per_sample = loss_function(outputs, X_batch)\n",
    "        weighted_loss = loss_per_sample\n",
    "        #Compute average loss across the batch\n",
    "        loss = weighted_loss\n",
    "        # print(f\"Batch Loss: {loss.item():.4f}\")\n",
    "        progress_bar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        #Update model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        all_probs.append(outputs.detach().cpu())\n",
    "        all_targets.append(X_batch.cpu())\n",
    "        all_weights.append(weights_batch.cpu())\n",
    "\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    all_weights = torch.cat(all_weights).numpy()\n",
    "    mse = mean_squared_error(all_targets, all_probs)\n",
    "    r2 = 0\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss, mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "61ca7925",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:01.841369Z",
     "iopub.status.busy": "2024-12-17T21:35:01.840879Z",
     "iopub.status.idle": "2024-12-17T21:35:01.847579Z",
     "shell.execute_reply": "2024-12-17T21:35:01.846774Z"
    },
    "papermill": {
     "duration": 0.017108,
     "end_time": "2024-12-17T21:35:01.849116",
     "exception": false,
     "start_time": "2024-12-17T21:35:01.832008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def evaluate_ae(model, loader, optimizer, loss_function, device):\n",
    "    model.train() #set model to training mode\n",
    "    #Iterate over batches \n",
    "    progress_bar = tqdm(loader, desc=\"Validating Progress\", leave=True,position =0)\n",
    "    total_loss = 0\n",
    "    all_probs = []\n",
    "    all_targets = []\n",
    "    all_weights = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch, batch_time,batch_date,weights_batch in progress_bar:\n",
    "            #Move data to specified device (CPU or GPU)\n",
    "            # X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            # weights_batch = weights_batch.to(device)\n",
    "            #Reset Gradient to 0\n",
    "            optimizer.zero_grad()\n",
    "            # print(batch_time.shape,batch_date.shape)\n",
    "            _,outputs = model(X_batch)\n",
    "            outputs = outputs\n",
    "            loss_per_sample = loss_function(outputs, X_batch)\n",
    "            weighted_loss = loss_per_sample\n",
    "            #Compute average loss across the batch\n",
    "            loss = weighted_loss\n",
    "            # print(f\"Batch Loss: {loss.item():.4f}\")\n",
    "            progress_bar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            all_probs.append(outputs.detach().cpu())\n",
    "            all_targets.append(X_batch.cpu())\n",
    "            all_weights.append(weights_batch.cpu())\n",
    "\n",
    "    all_probs = torch.cat(all_probs).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "    all_weights = torch.cat(all_weights).numpy()\n",
    "    mse = mean_squared_error(all_targets, all_probs, sample_weight=all_weights)\n",
    "    r2 = 0\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return mse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "745aef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "974f9c8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:01.866873Z",
     "iopub.status.busy": "2024-12-17T21:35:01.866377Z",
     "iopub.status.idle": "2024-12-17T21:35:01.871916Z",
     "shell.execute_reply": "2024-12-17T21:35:01.871222Z"
    },
    "papermill": {
     "duration": 0.016158,
     "end_time": "2024-12-17T21:35:01.873465",
     "exception": false,
     "start_time": "2024-12-17T21:35:01.857307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(targets,probs,range):\n",
    "    pred = probs[range[0]:range[1]]\n",
    "    gt = targets[range[0]:range[1]]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(gt, label='Ground Truth', color='royalblue', linewidth=2)\n",
    "    plt.style.use('dark_background')  # Use a dark background style\n",
    "    # Plot predictions\n",
    "    plt.plot(pred, label='Predictions', color='tomato', linestyle='--', linewidth=2)\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Ground Truth vs Predictions', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Index', fontsize=14)\n",
    "    plt.ylabel('Value', fontsize=14)\n",
    "    \n",
    "    # Add a grid\n",
    "    plt.grid(color='gray', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "    \n",
    "    # Add a legend\n",
    "    plt.legend(fontsize=12, loc='upper right')\n",
    "    \n",
    "    # Customize tick parameters\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5bd50",
   "metadata": {
    "papermill": {
     "duration": 0.008063,
     "end_time": "2024-12-17T21:35:01.889962",
     "exception": false,
     "start_time": "2024-12-17T21:35:01.881899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training Model\n",
    "Use very few epochs because like mentionned lots of times, LSTM are very proned to overfitting. Also, the dataset is very large, more epochs seem to be detrimental with this type of model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61ab5638",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:01.907224Z",
     "iopub.status.busy": "2024-12-17T21:35:01.906982Z",
     "iopub.status.idle": "2024-12-17T21:35:01.911716Z",
     "shell.execute_reply": "2024-12-17T21:35:01.910934Z"
    },
    "papermill": {
     "duration": 0.015059,
     "end_time": "2024-12-17T21:35:01.913203",
     "exception": false,
     "start_time": "2024-12-17T21:35:01.898144",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1af4e58e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:01.930811Z",
     "iopub.status.busy": "2024-12-17T21:35:01.930573Z",
     "iopub.status.idle": "2024-12-17T21:35:01.944792Z",
     "shell.execute_reply": "2024-12-17T21:35:01.944207Z"
    },
    "papermill": {
     "duration": 0.024673,
     "end_time": "2024-12-17T21:35:01.946305",
     "exception": false,
     "start_time": "2024-12-17T21:35:01.921632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp = MLP()\n",
    "mlp = mlp.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b2e61596",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:01.963905Z",
     "iopub.status.busy": "2024-12-17T21:35:01.963659Z",
     "iopub.status.idle": "2024-12-17T21:35:01.966855Z",
     "shell.execute_reply": "2024-12-17T21:35:01.966209Z"
    },
    "papermill": {
     "duration": 0.013742,
     "end_time": "2024-12-17T21:35:01.968420",
     "exception": false,
     "start_time": "2024-12-17T21:35:01.954678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encoder.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83ad578c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:01.985990Z",
     "iopub.status.busy": "2024-12-17T21:35:01.985749Z",
     "iopub.status.idle": "2024-12-17T21:35:01.991836Z",
     "shell.execute_reply": "2024-12-17T21:35:01.991104Z"
    },
    "papermill": {
     "duration": 0.016511,
     "end_time": "2024-12-17T21:35:01.993368",
     "exception": false,
     "start_time": "2024-12-17T21:35:01.976857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:22<00:00, 12.78it/s, Batch Loss=1.5115]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:52<00:00, 14.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6926 r2 0.0020408034324645996\n",
      "epoch 0 train loss 2.0494, train_r2 -0.3576, train_mse 0.8033, val_mse 0.6926, val_r2 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 14.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5239 r2 0.0007173418998718262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:21<00:00, 12.88it/s, Batch Loss=1.4544]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:51<00:00, 15.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6922 r2 0.0026026368141174316\n",
      "epoch 1 train loss 1.5536, train_r2 -0.0291, train_mse 0.6090, val_mse 0.6922, val_r2 0.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5236 r2 0.0013319849967956543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:14<00:00, 13.49it/s, Batch Loss=1.5610]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6920 r2 0.0029251575469970703\n",
      "epoch 2 train loss 1.5146, train_r2 -0.0032, train_mse 0.5936, val_mse 0.6920, val_r2 0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5235 r2 0.0014260411262512207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:16<00:00, 13.28it/s, Batch Loss=1.4080]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6918 r2 0.0031568408012390137\n",
      "epoch 3 train loss 1.5070, train_r2 0.0017, train_mse 0.5907, val_mse 0.6918, val_r2 0.0032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5229 r2 0.0026865601539611816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:14<00:00, 13.46it/s, Batch Loss=1.5088]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6916 r2 0.003424525260925293\n",
      "epoch 4 train loss 1.5049, train_r2 0.0032, train_mse 0.5899, val_mse 0.6916, val_r2 0.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5237 r2 0.0010923147201538086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:16<00:00, 13.31it/s, Batch Loss=1.4753]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:51<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6914 r2 0.003819406032562256\n",
      "epoch 5 train loss 1.5038, train_r2 0.0039, train_mse 0.5894, val_mse 0.6914, val_r2 0.0038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5231 r2 0.00221329927444458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:15<00:00, 13.38it/s, Batch Loss=1.4153]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:51<00:00, 15.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6908 r2 0.00467371940612793\n",
      "epoch 6 train loss 1.5028, train_r2 0.0046, train_mse 0.5890, val_mse 0.6908, val_r2 0.0047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5226 r2 0.003147721290588379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:15<00:00, 13.42it/s, Batch Loss=1.4486]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6905 r2 0.005022168159484863\n",
      "epoch 7 train loss 1.5019, train_r2 0.0051, train_mse 0.5887, val_mse 0.6905, val_r2 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5230 r2 0.002424895763397217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:17<00:00, 13.23it/s, Batch Loss=1.4801]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6902 r2 0.005468964576721191\n",
      "epoch 8 train loss 1.5010, train_r2 0.0058, train_mse 0.5883, val_mse 0.6902, val_r2 0.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5229 r2 0.0025649070739746094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:16<00:00, 13.26it/s, Batch Loss=1.5744]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6903 r2 0.0054184794425964355\n",
      "epoch 9 train loss 1.5003, train_r2 0.0062, train_mse 0.5881, val_mse 0.6903, val_r2 0.0054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:15<00:00, 13.38it/s, Batch Loss=1.3797]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:51<00:00, 15.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6897 r2 0.0062683820724487305\n",
      "epoch 10 train loss 1.4995, train_r2 0.0067, train_mse 0.5878, val_mse 0.6897, val_r2 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5227 r2 0.0030151009559631348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:16<00:00, 13.35it/s, Batch Loss=1.5087]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6897 r2 0.006215810775756836\n",
      "epoch 11 train loss 1.4992, train_r2 0.0070, train_mse 0.5876, val_mse 0.6897, val_r2 0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:15<00:00, 13.38it/s, Batch Loss=1.5297]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6896 r2 0.006408631801605225\n",
      "epoch 12 train loss 1.4983, train_r2 0.0075, train_mse 0.5873, val_mse 0.6896, val_r2 0.0064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5229 r2 0.0025780797004699707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:16<00:00, 13.34it/s, Batch Loss=1.4611]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:51<00:00, 15.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6894 r2 0.0066751837730407715\n",
      "epoch 13 train loss 1.4979, train_r2 0.0078, train_mse 0.5871, val_mse 0.6894, val_r2 0.0067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5230 r2 0.0024098753929138184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:15<00:00, 13.43it/s, Batch Loss=1.5286]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6893 r2 0.006770133972167969\n",
      "epoch 14 train loss 1.4974, train_r2 0.0081, train_mse 0.5869, val_mse 0.6893, val_r2 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5226 r2 0.0032060742378234863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:14<00:00, 13.46it/s, Batch Loss=1.3453]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:51<00:00, 15.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6892 r2 0.007004797458648682\n",
      "epoch 15 train loss 1.4969, train_r2 0.0085, train_mse 0.5867, val_mse 0.6892, val_r2 0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5226 r2 0.003184497356414795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:16<00:00, 13.31it/s, Batch Loss=1.3803]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6890 r2 0.007249772548675537\n",
      "epoch 16 train loss 1.4963, train_r2 0.0089, train_mse 0.5865, val_mse 0.6890, val_r2 0.0072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5225 r2 0.003412187099456787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:14<00:00, 13.53it/s, Batch Loss=1.3945]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:51<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6892 r2 0.006910741329193115\n",
      "epoch 17 train loss 1.4961, train_r2 0.0090, train_mse 0.5864, val_mse 0.6892, val_r2 0.0069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:14<00:00, 13.47it/s, Batch Loss=1.6719]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6889 r2 0.007334411144256592\n",
      "epoch 18 train loss 1.4958, train_r2 0.0092, train_mse 0.5863, val_mse 0.6889, val_r2 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5226 r2 0.0032805800437927246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:15<00:00, 13.44it/s, Batch Loss=1.6656]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6890 r2 0.007259786128997803\n",
      "epoch 19 train loss 1.4954, train_r2 0.0095, train_mse 0.5861, val_mse 0.6890, val_r2 0.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:15<00:00, 13.43it/s, Batch Loss=1.2113]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:51<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6887 r2 0.007693350315093994\n",
      "epoch 20 train loss 1.4949, train_r2 0.0098, train_mse 0.5860, val_mse 0.6887, val_r2 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5229 r2 0.002688288688659668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:15<00:00, 13.36it/s, Batch Loss=1.5887]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:51<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6885 r2 0.007934331893920898\n",
      "epoch 21 train loss 1.4949, train_r2 0.0098, train_mse 0.5859, val_mse 0.6885, val_r2 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5228 r2 0.0028008222579956055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:15<00:00, 13.36it/s, Batch Loss=1.4563]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6886 r2 0.00783383846282959\n",
      "epoch 22 train loss 1.4943, train_r2 0.0102, train_mse 0.5857, val_mse 0.6886, val_r2 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:15<00:00, 13.45it/s, Batch Loss=1.3055]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6884 r2 0.008138120174407959\n",
      "epoch 23 train loss 1.4939, train_r2 0.0104, train_mse 0.5856, val_mse 0.6884, val_r2 0.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5225 r2 0.003347933292388916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:15<00:00, 13.36it/s, Batch Loss=1.6362]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:51<00:00, 15.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6884 r2 0.008122384548187256\n",
      "epoch 24 train loss 1.4939, train_r2 0.0105, train_mse 0.5855, val_mse 0.6884, val_r2 0.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:16<00:00, 13.28it/s, Batch Loss=1.4163]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6881 r2 0.008478820323944092\n",
      "epoch 25 train loss 1.4934, train_r2 0.0108, train_mse 0.5854, val_mse 0.6881, val_r2 0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5229 r2 0.0026984214782714844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:14<00:00, 13.55it/s, Batch Loss=1.5727]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:51<00:00, 15.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6884 r2 0.00813525915145874\n",
      "epoch 26 train loss 1.4931, train_r2 0.0110, train_mse 0.5852, val_mse 0.6884, val_r2 0.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:14<00:00, 13.47it/s, Batch Loss=1.5155]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6881 r2 0.008519530296325684\n",
      "epoch 27 train loss 1.4929, train_r2 0.0112, train_mse 0.5851, val_mse 0.6881, val_r2 0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5227 r2 0.002936244010925293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:14<00:00, 13.49it/s, Batch Loss=1.5892]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6882 r2 0.008404850959777832\n",
      "epoch 28 train loss 1.4925, train_r2 0.0114, train_mse 0.5850, val_mse 0.6882, val_r2 0.0084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 1816/1816 [02:15<00:00, 13.36it/s, Batch Loss=1.3884]\n",
      "Validating Progress: 100%|██████████| 777/777 [00:50<00:00, 15.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.6881 r2 0.008597612380981445\n",
      "epoch 29 train loss 1.4922, train_r2 0.0115, train_mse 0.5849, val_mse 0.6881, val_r2 0.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating Progress: 100%|██████████| 83/83 [00:05<00:00, 15.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss val: 0.5227 r2 0.0030341744422912598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = optim.Adam(mlp.parameters(), lr=1e-4) #Adam optimizer\n",
    "loss_function = nn.MSELoss(reduction='none')\n",
    "epochs = 30\n",
    "best = float('-inf')\n",
    "degraded = 0\n",
    "best_model = mlp\n",
    "if evaluate:\n",
    "    targets,probs,val_mse, val_r2 = evaluate_model(mlp,None, test_loader,generate_preds=evaluate)\n",
    "else:\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_mse, train_r2 = train_model(mlp,None, train_loader, optimizer, loss_function, device)\n",
    "        val_mse, val_r2 = evaluate_model(mlp,None, val_loader)\n",
    "    \n",
    "        print(f'epoch {epoch} train loss {train_loss:.4f}, train_r2 {train_r2:.4f}, train_mse {train_mse:.4f}, val_mse {val_mse:.4f}, val_r2 {val_r2:.4f}')\n",
    "        if val_r2 > best:\n",
    "            best = val_r2\n",
    "            best_model = copy.deepcopy(mlp)\n",
    "            torch.save(best_model.state_dict(), f'torchcustomwithencoder.pth')\n",
    "            degraded = 0\n",
    "            targets,probs,val_mse, val_r2 = evaluate_model(mlp, None,test_loader,generate_preds=True)\n",
    "            # plot(targets,probs,[245,455])\n",
    "        else:\n",
    "            degraded += 1\n",
    "        if degraded > 5:\n",
    "            break\n",
    "    model = mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5387524b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:02.011027Z",
     "iopub.status.busy": "2024-12-17T21:35:02.010780Z",
     "iopub.status.idle": "2024-12-17T21:35:02.013904Z",
     "shell.execute_reply": "2024-12-17T21:35:02.013227Z"
    },
    "papermill": {
     "duration": 0.013609,
     "end_time": "2024-12-17T21:35:02.015450",
     "exception": false,
     "start_time": "2024-12-17T21:35:02.001841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41d98237",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:02.033207Z",
     "iopub.status.busy": "2024-12-17T21:35:02.032891Z",
     "iopub.status.idle": "2024-12-17T21:35:02.036657Z",
     "shell.execute_reply": "2024-12-17T21:35:02.035866Z"
    },
    "papermill": {
     "duration": 0.014274,
     "end_time": "2024-12-17T21:35:02.038094",
     "exception": false,
     "start_time": "2024-12-17T21:35:02.023820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# if True:\n",
    "#     plot(targets,probs,[1000*i,1000*(i+1)])\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33130135",
   "metadata": {
    "papermill": {
     "duration": 0.008243,
     "end_time": "2024-12-17T21:35:02.054681",
     "exception": false,
     "start_time": "2024-12-17T21:35:02.046438",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Upload model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a83c12f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:02.072371Z",
     "iopub.status.busy": "2024-12-17T21:35:02.071897Z",
     "iopub.status.idle": "2024-12-17T21:35:02.075154Z",
     "shell.execute_reply": "2024-12-17T21:35:02.074381Z"
    },
    "papermill": {
     "duration": 0.013664,
     "end_time": "2024-12-17T21:35:02.076684",
     "exception": false,
     "start_time": "2024-12-17T21:35:02.063020",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save only the model's parameters \n",
    "# if run_type == 'Interactive':\n",
    "#     torch.save(model.state_dict(), 'lstm_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42cda4b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:02.094438Z",
     "iopub.status.busy": "2024-12-17T21:35:02.094185Z",
     "iopub.status.idle": "2024-12-17T21:35:02.097603Z",
     "shell.execute_reply": "2024-12-17T21:35:02.096831Z"
    },
    "papermill": {
     "duration": 0.014123,
     "end_time": "2024-12-17T21:35:02.099096",
     "exception": false,
     "start_time": "2024-12-17T21:35:02.084973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(\"hrerer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556cf8ca",
   "metadata": {
    "papermill": {
     "duration": 0.008348,
     "end_time": "2024-12-17T21:35:02.116148",
     "exception": false,
     "start_time": "2024-12-17T21:35:02.107800",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09e134c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:02.134117Z",
     "iopub.status.busy": "2024-12-17T21:35:02.133877Z",
     "iopub.status.idle": "2024-12-17T21:35:02.137366Z",
     "shell.execute_reply": "2024-12-17T21:35:02.136604Z"
    },
    "papermill": {
     "duration": 0.014204,
     "end_time": "2024-12-17T21:35:02.138856",
     "exception": false,
     "start_time": "2024-12-17T21:35:02.124652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the model's parameters\n",
    "# if run_type == 'Interactive':\n",
    "#     input_size = 79\n",
    "#     hidden_size = 32\n",
    "#     output_size = 1\n",
    "#     time_steps = 1\n",
    "#     loaded_model = LiquidSNN(input_size, hidden_size, output_size, time_steps)\n",
    "#     loaded_model.load_state_dict(torch.load('/kaggle/input/snn_liquid/pytorch/default/1/torchlstm.pth',weights_only=True))\n",
    "\n",
    "#     # loaded_model = LSTM(input_size=79, hidden_dim=hidden_dim, output_size=1, num_layers=num_layer) # Recreate the model structure\n",
    "#     # loaded_model.load_state_dict(torch.load('lstm_model_weights.pth'))\n",
    "#     loaded_model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9270735",
   "metadata": {
    "papermill": {
     "duration": 0.008261,
     "end_time": "2024-12-17T21:35:02.155380",
     "exception": false,
     "start_time": "2024-12-17T21:35:02.147119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "379ac70c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:02.174141Z",
     "iopub.status.busy": "2024-12-17T21:35:02.173894Z",
     "iopub.status.idle": "2024-12-17T21:35:02.178228Z",
     "shell.execute_reply": "2024-12-17T21:35:02.177445Z"
    },
    "papermill": {
     "duration": 0.015712,
     "end_time": "2024-12-17T21:35:02.179828",
     "exception": false,
     "start_time": "2024-12-17T21:35:02.164116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "#     \"\"\"Make a prediction.\"\"\"\n",
    "#     global model\n",
    "#     global device\n",
    "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     # Load the model if it's not already loaded\n",
    "\n",
    "    \n",
    "#     model = LiquidSNN(input_size, hidden_size, output_size, time_steps)\n",
    "#     model.load_state_dict(torch.load('/kaggle/input/snn_liquid/pytorch/default/1/torchlstm.pth',weights_only=True))\n",
    "#     model = model.to(device)\n",
    "#     # loaded_model = LSTM(input_size=79, hidden_dim=hidden_dim, output_size=1, num_layers=num_layer) # Recreate the model structure\n",
    "#     # loaded_model.load_state_dict(torch.load('lstm_model_weights.pth'))\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     sel_cols  = [f\"feature_{i:02d}\" for i in range(79)]\n",
    "\n",
    "#     # Process the test data using Polars\n",
    "#     # Select the same features used during training\n",
    "\n",
    "\n",
    "#     # Ensure all required columns are present\n",
    "#     missing_cols = set(sel_cols) - set(test.columns)\n",
    "#     if missing_cols:\n",
    "#         raise ValueError(f\"Missing columns in test data: {missing_cols}\")\n",
    "\n",
    "#     # Select the features\n",
    "#     test_features = test.select(sel_cols)\n",
    "\n",
    "#     # **Apply forward fill and then fill remaining missing values with zero**\n",
    "#     test_features = test_features.fill_null(strategy='forward').fill_null(0)\n",
    "#     test_features = normalize_dataframe(test_features,means,stds)\n",
    "\n",
    "#     # Convert Polars DataFrame to NumPy array\n",
    "#     X_test = test_features.to_numpy()\n",
    "\n",
    "#     # Convert to Torch tensor\n",
    "#     X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "#     # Make predictions\n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         outputs = model(X_test_tensor)\n",
    "#         # Assuming the model outputs a tensor of shape (batch_size, 1)\n",
    "#         predictions = outputs.squeeze().cpu().numpy()\n",
    "\n",
    "#     # Prepare the predictions DataFrame\n",
    "#     predictions_df = pl.DataFrame({\n",
    "#         'row_id': test['row_id'],\n",
    "#         'responder_6': predictions\n",
    "#     })\n",
    "\n",
    "#     # The predict function must return a DataFrame\n",
    "#     assert isinstance(predictions_df, (pl.DataFrame, pd.DataFrame))\n",
    "#     # with columns 'row_id', 'responder_6'\n",
    "#     assert predictions_df.columns == ['row_id', 'responder_6']\n",
    "#     # and as many rows as the test data.\n",
    "#     assert len(predictions_df) == len(test)\n",
    "\n",
    "#     return predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "49932994",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T21:35:02.197754Z",
     "iopub.status.busy": "2024-12-17T21:35:02.197234Z",
     "iopub.status.idle": "2024-12-17T21:35:02.200752Z",
     "shell.execute_reply": "2024-12-17T21:35:02.200073Z"
    },
    "papermill": {
     "duration": 0.014082,
     "end_time": "2024-12-17T21:35:02.202231",
     "exception": false,
     "start_time": "2024-12-17T21:35:02.188149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# import polars as pl\n",
    "\n",
    "# import kaggle_evaluation.jane_street_inference_server\n",
    "\n",
    "\n",
    "# inference_server = \\\n",
    "# kaggle_evaluation.jane_street_inference_server.JSInferenceServer(predict)\n",
    "\n",
    "# if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "#     inference_server.serve()\n",
    "# else:\n",
    "#     inference_server.run_local_gateway(\n",
    "#         (\n",
    "#             '/kaggle/input/jane-street-real-time-market-data-forecasting/test.parquet',\n",
    "#             '/kaggle/input/jane-street-real-time-market-data-forecasting/lags.parquet',\n",
    "#         )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0577c2a",
   "metadata": {
    "papermill": {
     "duration": 0.008269,
     "end_time": "2024-12-17T21:35:02.218950",
     "exception": false,
     "start_time": "2024-12-17T21:35:02.210681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "jane_street",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 69.389151,
   "end_time": "2024-12-17T21:35:04.843528",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-17T21:33:55.454377",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
